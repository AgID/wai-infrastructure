---
# Deploy K8S WAI Resources
- hosts: kube-master[0]
  collections:
    - kubernetes.core

  tasks:
    - include_vars:
        file: secrets.yml

    # Base objects
    - name: K8S .:. Create destination directory
      file:
        path: "{{ wai_k8s_deployment_dir }}/helm"
        state: directory

      # Install pyhton3
    - name: K8S Helm .:. Install python
      apt:
        update_cache: true
        name:
          - python3
          - git
          - python3-pip
          - python3-pygit2
        state: present

    # Install PYHelm
    - name: K8S Helm .:. Install requirements
      pip:
        name:
          - pyhelm==2.14.5
          - grpcio

    - name: K8S Helm .:. Add Jetstack repo
      kubernetes.core.helm_repository:
        repo_name: jetstack
        repo_url: "https://charts.jetstack.io"

    - name: K8S Helm .:. Add prometheus chart repo
      kubernetes.core.helm_repository:
        repo_name: prometheus-community 
        repo_url: https://prometheus-community.github.io/helm-charts

    - name: K8S Helm .:. Add grafana chart repo
      kubernetes.core.helm_repository:
        repo_name: grafana
        repo_url: https://grafana.github.io/helm-charts

    - name: K8S Helm .:. Add bitnami chart repo
      kubernetes.core.helm_repository:
        repo_name: bitnami
        repo_url: https://charts.bitnami.com/bitnami

    - name: K8S Helm .:. Install CertManager
      kubernetes.core.helm:
        release_namespace: cert-manager
        create_namespace: true
        name: cert-manager
        chart_ref: jetstack/cert-manager
        release_state: present
        values:
          installCRDs: true

    - name: K8S Helm .:. Install Prometheus
      kubernetes.core.helm:
        release_namespace: monitoring
        create_namespace: true
        name: prometheus
        chart_ref: prometheus-community/prometheus
        release_state: present
        values:
          server:
            ingress:
              enabled: true
              hosts:
                - prometheus.wai.local
              annotations:
                kubernetes.io/ingress.class: nginx-internal
                nginx.ingress.kubernetes.io/ssl-redirect: "false"
                nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
            persistentVolume:
              enabled: true
              size: 150Gi
              storageClass: storage-heketi
          networkPolicy:
            enabled: false
          alertmanager:
            persistentVolume:
              enabled: true
              size: 2Gi
              storageClass: storage-heketi
          alertmanagerFiles:
            alertmanager.yml:
              global:
                slack_api_url: "{{ prometheus_alert_slack_web_hook_url }}"
              receivers:
                - name: default-receiver
                  slack_configs:
                    - channel: "#wai"
                      send_resolved: true
                      title: "{{ '{{' }} range .Alerts {{ '}}' }}{{ '{{' }} .Annotations.summary {{ '}}' }}\n{{ '{{' }} end {{ '}}' }}"
                      text: "{{ '{{' }} range .Alerts {{ '}}' }}{{ '{{' }} .Annotations.description {{ '}}' }}}}\n{{ '{{' }} end {{ '}}' }}"
              route:
                group_wait: 10s
                group_interval: 5m
                receiver: default-receiver
                repeat_interval: 3h
          extraScrapeConfigs: |
            - job_name: wai-infrastructure
              static_configs:
              - targets:
            {{ groups['wai'] | difference(groups['k8s-cluster']) | map('extract', hostvars, ['ansible_default_ipv4', 'address']) | map('regex_replace', '^(.*)$','\1:9100') | list | to_nice_yaml | indent(4, true) }}
            - job_name: wai-mariadb
              static_configs:
              - targets:
            {{ groups['galera-prod'] | union(groups['galera-prod-slave']) | union(groups['galera-play']) | union(groups['mariadb-stag']) | map('extract', hostvars, ['ansible_default_ipv4', 'address']) | map('regex_replace', '^(.*)$','\1:9104') | list | to_nice_yaml | indent(4, true) }}
          serverFiles:
            alerting_rules.yml:
              groups:
                - name: Redis
                  rules:
                    - alert: RedisMemory
                      expr: redis_memory_used_bytes / redis_config_maxmemory * 100 > 80
                      for: 1m
                      labels:
                        severity: critical
                      annotations:
                        description: "Redis pod {{ '{{$labels.kubernetes_pod_name}}' }} of namespace {{ '{{$labels.kubernetes_namespace}}' }} is consuming too much memory. Percentage is {{ '{{$value}}' }}"
                    - alert: RedisDown
                      expr: redis_up == 0
                      for: 5m
                      labels:
                        severity: critical
                      annotations:
                        description: "Redis instance {{ '{{$labels.kubernetes_pod_name}}' }} in namespace {{ '{{$labels.kubernetes_namespace}}' }} has been down for more than 5 minutes."
                - name: MariaDBInstance
                  rules:
                    - alert: MariaDown
                      expr: mysql_up == 0
                      for: 5m
                      labels:
                        severity: critical
                      annotations:
                        description: "MariaDB instance {{ '{{$labels.instance}}' }} of job {{ '{{$labels.job}}' }} has been down for more than 5 minutes."
                    - alert: SlaveSQLNotRunnin
                      annotations:
                        description: Slave SQL not running on galera cluster slave node
                        summary: Slave SQL not running
                      expr: mysql_slave_status_slave_sql_running==0 or absent(mysql_slave_status_slave_sql_running)
                      for: 1m
                      labels:
                        severity: critical
                    - alert: SlaveIONotRunnin
                      annotations:
                        description: Slave IO not running on galera cluster slave node
                        summary: Slave IO not running
                      expr: mysql_slave_status_slave_io_running==0 or absent(mysql_slave_status_slave_io_running)
                      for: 1m
                      labels:
                        severity: critical
                - name: Ingress
                  rules:
                    - alert: NginxErrorCode
                      annotations:
                        description: "{{ '{{$labels.ingress}}' }} http status code =~[5].* > 1.0 %, current value is {{ '{{$value}}' }}"
                        summary: http status code
                      expr: sum by (ingress) (rate(nginx_ingress_controller_requests{namespace="wai-prod",status=~"[5].*"}[5m])) / sum by (ingress) (rate(nginx_ingress_controller_requests{namespace="wai-prod"}[5m])) * 100 > 1.0
                      for: 1m
                      labels:
                        severity: critical
                - name: IstanceAlert
                  rules:
                    - alert: InstanceNoK8sDown
                      expr: up{job="wai-infrastructure"} == 0
                      for: 5m
                      labels:
                        severity: critical
                      annotations:
                        description: "{{ '{{$labels.instance}}' }} of job {{ '{{$labels.job}}' }} has been down for more than 5 minutes."
                    - alert: InstanceK8sDown
                      expr: up{job="kubernetes-nodes"} == 0
                      for: 5m
                      labels:
                        severity: critical
                      annotations:
                        description: "{{ '{{$labels.instance}}' }} of job {{ '{{$labels.job}}' }} has been down for more than 5 minutes."
                    - alert: InstanceCPUUsageNoK8S
                      expr: (100 - (avg(irate(node_cpu_seconds_total{mode="idle", job="wai-infrastructure"}[5m])) BY (instance) * 100)) > 75
                      for: 2m
                      labels:
                        severity: critical
                      annotations:
                        description: "{{ '{{$labels.instance}}' }}: CPU usage is above 75% (current value is:{{ '{{$value}}' }})"
                    - alert: InstanceCPUUsageK8S
                      expr: (100 - (avg(irate(node_cpu_seconds_total{mode="idle", job="kubernetes-service-endpoints"}[5m])) BY (instance) * 100)) > 75
                      for: 2m
                      labels:
                        severity: critical
                      annotations:
                        description: "{{ '{{$labels.kubernetes_node}}' }}: CPU usage is above 75% (current value is: {{ '{{$value}}' }})"
                - name: Kubernetes
                  rules:
                    - alert: AvailableReplicas
                      annotations:
                        description: "Missing {{ '{{$value}}' }} replicas of deployment {{ '{{$labels.deployment}}' }} in namespace {{ '{{$labels.namespace}}' }}"
                        summary: Unavailable replica
                      expr: kube_deployment_status_replicas_unavailable > 0
                      for: 1m
                      labels:
                        severity: critical

    - name: K8S Helm .:. Install Grafana
      kubernetes.core.helm:
        release_namespace: monitoring
        name: grafana
        chart_ref: grafana/grafana
        release_state: present
        values:
          ingress:
            enabled: true
            hosts:
              - grafana.wai.local
            annotations:
              kubernetes.io/ingress.class: nginx-internal
              nginx.ingress.kubernetes.io/ssl-redirect: "false"
              nginx.ingress.kubernetes.io/force-ssl-redirect: "false"
          persistence:
            enabled: true
            size: 8Gi
            storageClassName: storage-heketi
          initChownData:
            enabled: false
          resources:
            limits:
              cpu: "0.25"
              memory: "1024Mi"
            requests:
              cpu: "0.25"
              memory: "512Mi"
          datasources:
            datasources.yaml:
              apiVersion: 1
              datasources:
                - name: Prometheus
                  access: proxy
                  orgId: 1
                  type: prometheus
                  url: http://prometheus-server
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
                - name: "default"
                  orgId: 1
                  folder: ""
                  type: file
                  disableDeletion: false
                  editable: true
                  options:
                    path: /var/lib/grafana/dashboards/default
          dashboards:
            default:
              kubernetes-cluster:
                gnetId: 7249
                revision: 1
                datasource: Prometheus
              nginx-ingress:
                gnetId: 9614
                revision: 51
                datasource: Prometheus
              kubernetes-cluster-monitoring-prometheus:
                gnetId: 8588
                revision: 1
                datasource: Prometheus
              node-exporter-full:
                gnetId: 1860
                revision: 15
                datasource: Prometheus
              redis-full:
                gnetId: 763
                revision: 3
                datasource: Prometheus
              mariadb-full:
                gnetId: 7362
                revision: 5
                datasource: Prometheus

    # Install Redis
    - name: K8S Helm .:. Install Redis
      kubernetes.core.helm:
        release_namespace: "{{ item[0] }}"
        name: "{{ item[1] }}-{{ item[0] }}"
        chart_ref: bitnami/redis
        release_state: present
        values:
          image:
            repository: "{{ wai_redisearch_image }}"
            tag: "{{ 'latest' if item[0] == 'wai-stag' else wai_redisearch_image_tag }}"
          cluster:
            enabled: false
          usePassword: true
          existingSecret: redis-master-secret
          existingSecretPasswordKey: "{{ item[1] }}-password"
          fullnameOverride: "{{ item[1] }}"
          metrics:
            enabled: true
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 100m
                memory: 128Mi
          master:
            extraFlags:
              - "--loadmodule /opt/bitnami/redis/bin/redisearch.so"
              - "--maxmemory {{ '7500' if item[1] == 'ingestion-redis' else '256' }}mb"
              - "--maxmemory-policy noeviction"
            disableCommands: "{{ ['FLUSHDB', 'FLUSHALL'] if item[0] == 'wai-prod' else []}}"
            persistence:
              enabled: true
              storageClass: storage-heketi
              size: 5Gi
            resources:
              limits:
                cpu: 250m
                memory: "{{ '4Gi' if item[1] == 'ingestion-redis' else '2Gi' }}"
              requests:
                cpu: 250m
                memory: "{{ '4Gi' if item[1] == 'ingestion-redis' else '1Gi' }}"
      with_nested:
        - "{{ wai_k8s_namespaces }}"
        - ["ingestion-redis", "application-redis"]

    # Install IPA Redis Search
    - name: K8S Helm .:. Install IPA Redisearch
      kubernetes.core.helm:
        release_namespace: "{{ item }}"
        release_name: "ipa-redisearch-{{ item }}"
        chart_version: 10.5.4
        chart_ref: bitnami/redis
        release_state: present
        values:
          image:
            repository: "{{ wai_ipa_redisearch_image }}"
            tag: "{{ 'latest' if item[0] == 'wai-stag' else wai_ipa_redisearch_image_tag }}"
          cluster:
            enabled: false
          usePassword: true
          existingSecret: redis-master-secret
          existingSecretPasswordKey: ipa-redisearch-password
          fullnameOverride: ipa-redisearch
          securityContext:
            runAsUser: 0
          metrics:
            enabled: true
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 100m
                memory: 128Mi
          master:
            persistence:
              enabled: false
              storageClass: storage-heketi
              size: 2Gi
            resources:
              limits:
                cpu: 250m
                memory: 512Mi
              requests:
                cpu: 250m
                memory: 256Mi
            extraFlags:
              - "--maxmemory 256mb"
              - "--maxmemory-policy noeviction"
      with_items:
        - "{{ wai_k8s_namespaces }}"

    # Install postgres
    - name: K8S Helm .:. Install Kong Postgres
      kubernetes.core.helm:
        release_namespace: "{{ item }}"
        create_namespace: true
        name: kong-postgres
        chart_ref: bitnami/postgresql
        release_state: present
        values:
          image:
            tag: 11
          fullnameOverride: kong-pgsql
          persistence:
            storageClass: storage-heketi
          postgresqlPostgresPassword: "{{ kong[item].pgsql.postgres_password }}"
          postgresqlUsername: "{{ kong[item].pgsql.user }}"
          postgresqlPassword: "{{ kong[item].pgsql.password }}"
          postgresqlDatabase: kong
          resources:
            requests:
              cpu: "1"
              memory: 1G
            limits:
              cpu: "1"
              memory: 1G

          metrics:
            serviceMonitor:
              enabled: true
      with_items:
        - "{{ wai_k8s_namespaces }}"

    # At the end.
    - name: K8S Helm .:. Prepare cluster issuer
      template:
        dest: "{{ wai_k8s_deployment_dir }}/helm/01_cluster-issuer-{{ item }}.yml"
        src: k8s/helm/01_cluster-issuer.yml.j2
      with_items:
        - "{{ wai_k8s_namespaces }}"

    - name: K8S Helm .:. Deploy cluster issuer
      kube:
        name: "k8s-deployment"
        kubectl: "{{ wai_kubectl }}"
        state: "latest"
        filename: "{{ wai_k8s_deployment_dir }}/helm/01_cluster-issuer-{{ item }}.yml"
      with_items:
        - "{{ wai_k8s_namespaces }}"
        - internal
      register: result
      until: result == 0
      retries: 6
      delay: 20

    - name: K8S Helm .:. Cleanup
      file:
        state: absent
        path: "{{ wai_k8s_deployment_dir }}/helm/"
